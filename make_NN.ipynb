{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba617bee",
   "metadata": {},
   "source": [
    "# make neural network with numpy and math lib\n",
    "Designing a neural network to predict the behavior of the sinusoidal function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4670b5",
   "metadata": {},
   "source": [
    "## import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595729ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc62d917",
   "metadata": {},
   "source": [
    "## make random input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf0aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96600ab",
   "metadata": {},
   "source": [
    "## Randomly initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c161024",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f964577",
   "metadata": {},
   "source": [
    "## cycle learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe224ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lerning_rate = 1e-6 # 10^-6\n",
    "for t in range(2000):\n",
    "    # formol: y = a + b * x + c * x^2 + d * x^3\n",
    "    y_pred = a + (b * x) + (c * (x ** 2)) + (d * (x ** 3))\n",
    "\n",
    "    # comput loss per tern\n",
    "    loss = np.square(y_pred - y).sum() # Mean Squared Error (MSE)\n",
    "    if t % 100 == 99: # print per 100 tern\n",
    "        print(t, loss)\n",
    "\n",
    "    # backprop, comput gradients for a, b, c, d\n",
    "    grad_y_pred = 2 * (y_pred - y) # The reason for using this relation is to derive the derivative of the loss function.\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * (x ** 2)).sum()\n",
    "    grad_d = (grad_y_pred * (x ** 3)).sum()\n",
    "\n",
    "    # update a, b, c, d\n",
    "    a -= lerning_rate * grad_a\n",
    "    b -= lerning_rate * grad_b\n",
    "    c -= lerning_rate * grad_c\n",
    "    d -= lerning_rate * grad_d\n",
    "\n",
    "print(F\"result: y = {a} + {b} * x + {c} * x^2 + {d} * x^3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
